{
    "": "",
    "translated = ": "translated =",
    "Fig. 1 presents an illustrative example. As the weight of\nthe edge between vertex 6 and 7 is large, i.e., 6 and 7 have a\nhigh first-order proximity, they should be represented closely\nto each other in the embedded space. On the other hand,\nthough there is no link between vertex 5 and 6, they share\nmany common neighbors, i.e., they have a high second-order\nproximity and therefore should also be represented closely to\neach other. We expect that the consideration of the second-\norder proximity effectively complements the sparsity of the\nfirst-order proximity and better preserves the global struc-\nture of the network. In this paper, we will present care-\nfully designed objectives that preserve the first-order and\nthe second-order proximities.": "图1示出了说明性示例。由于顶点6和顶点7之间的边的权重很大，即6和7具有较高的一阶接近度，因此在嵌入空间中它们应该相互紧密地表示。另一方面，虽然顶点5和6之间没有联系，但它们有许多共同的邻居，即它们具有很高的二阶邻近性，因此也应该彼此紧密地表示。我们期望对二阶邻近度的考虑有效地补充了一阶邻近度的稀疏性，更好地保持了网络的全局结构。在本文中，我们将提出精心设计的目标，以保持一阶和二阶近似值.",
    "Fig. 1 presents an illustrative example. As the weight of the edge between vertex 6 and 7 is large, i.e., 6 and 7 have a high first-order proximity, they should be represented closely to each other in the embedded space. On the other hand, though there is no link between vertex 5 and 6, they share many common neighbors, i.e., they have a high second-order proximity and therefore should also be represented closely to each other. We expect that the consideration of the secondorder proximity effectively complements the sparsity of the first-order proximity and better preserves the global structure of the network. In this paper, we will present carefully designed objectives that preserve the first-order and the second-order proximities.": "图1示出了说明性示例。由于顶点6和顶点7之间的边的权重很大，即6和7具有较高的一阶接近度，因此在嵌入空间中它们应该相互紧密地表示。另一方面，虽然顶点5和6之间没有联系，但它们有许多共同的邻居，即它们具有很高的二阶邻近性，因此也应该彼此紧密地表示。我们期望对二阶邻近度的考虑有效地补充了一阶邻近度的稀疏性，更好地保持了网络的全局结构。在本文中，我们将提出精心设计的目标，以保持一阶和二阶近似值.",
    "translated = {}": "translated = {}",
    "Even if a sound objective is found, optimizing it for a very\nlarge network is challenging. One approach that attracts\nattention in recent years is using the stochastic gradient de-\nscent for the optimization. However, we show that directly\ndeploying the stochastic gradient descent is problematic for\nreal world information networks. This is because in many\nnetworks, edges are weighted and the weights usually present\na high variance. Consider a word co-occurrence network, in\nwhich the weights (co-occurrences) of word pairs may range\nfrom one to hundreds of thousands. These weights of the\nedges will be multiplied into the gradients, resulting in the\nexplosion of the gradients and thus compromise the perfor-\nmance. To address this, we propose a novel edge-sampling\nmethod, which improves both the effectiveness and efficiency\nof the inference. We sample the edges with the probabilities\nproportional to their weights, and then treat the sampled\nedges as binary edges for model updating. With this sam-\npling process, the objective function remains the same and\nthe weights of the edges no longer affect the gradients.": "即使找到了一个合理的目标，为一个非常大的网络优化它也是很有挑战性的。利用随机梯度下降进行优化是近年来备受关注的一种方法。然而，我们证明了直接部署随机梯度下降对于现实世界的信息网络是有问题的。这是因为在许多网络中，边是加权的，而权值通常具有很高的方差。考虑一个单词共生网络，在这个网络中，单词对的权重(同时出现)可能从1到几十万不等。这些边的权重将被乘以梯度，导致梯度的爆炸，从而损害性能。针对这一问题，我们提出了一种新的边缘采样方法，提高了推理的有效性和效率。我们用概率与其权重成正比的概率对边缘进行采样，然后将采样的边缘作为二值化的边缘进行模型更新。在这种采样过程中，目标函数保持不变，边的权重不再影响梯度。",
    "Even if a sound objective is found, optimizing it for a very large network is challenging. One approach that attracts attention in recent years is using the stochastic gradient descent for the optimization. However, we show that directly deploying the stochastic gradient descent is problematic for real world information networks. This is because in many networks, edges are weighted and the weights usually present a high variance. Consider a word co-occurrence network, in which the weights (co-occurrences) of word pairs may range from one to hundreds of thousands. These weights of the edges will be multiplied into the gradients, resulting in the explosion of the gradients and thus compromise the performance. To address this, we propose a novel edge-sampling method, which improves both the effectiveness and efficiency of the inference. We sample the edges with the probabilities proportional to their weights, and then treat the sampled edges as binary edges for model updating. With this sampling process, the objective function remains the same and the weights of the edges no longer affect the gradients.": "即使找到了一个合理的目标，为一个非常大的网络优化它也是很有挑战性的。利用随机梯度下降进行优化是近年来备受关注的一种方法。然而，我们证明了直接部署随机梯度下降对于现实世界的信息网络是有问题的。这是因为在许多网络中，边是加权的，而权值通常具有很高的方差。考虑一个单词共生网络，在这个网络中，单词对的权重(同时出现)可能从1到几十万不等。这些边的权重将被乘以梯度，导致梯度的爆炸，从而损害性能。针对这一问题，我们提出了一种新的边缘采样方法，提高了推理的有效性和效率。我们用概率与其权重成正比的概率对边缘进行采样，然后将采样的边缘作为二值化的边缘进行模型更新。在这种采样过程中，目标函数保持不变，边的权重不再影响梯度。",
    "usually": "adv. 通常，经常，平常，惯常地;一直，向来;动不动，一般;素;",
    "proportional": "adj. 比例的，成比例的;相称的，均衡的;\nn. [数]比例项，比例量;",
    "The LINE is very general, which works well for directed\nor undirected, weighted or unweighted graphs. We evalu-\nate the performance of the LINE with various real-world\ninformation networks, including language networks, social\nnetworks, and citation networks. The effectiveness of the\nlearned embeddings is evaluated within multiple data min-\ning tasks, including word analogy, text classification, and\nnode classification. The results suggest that the LINE model\noutperforms other competitive baselines in terms of both ef-\nfectiveness and efficiency. It is able to learn the embedding\nof a network with millions of nodes and billions of edges in\na few hours on a single machine.": "这条线是非常通用的，它适用于有向或无向、加权或非加权图。我们用各种真实世界的信息网络，包括语言网络、社交网络和引文网络来评估这条线路的性能。在多个数据挖掘任务(包括单词类推、文本分类和节点分类)中评估所学习的嵌入的有效性。结果表明，该线性模型在有效性和效率方面均优于其他竞争基线。它能够在一台机器上几个小时内学习数百万个节点和数十亿个边的网络的嵌入。",
    "The LINE is very general, which works well for directed or undirected, weighted or unweighted graphs. We evaluate the performance of the LINE with various real-world information networks, including language networks, social networks, and citation networks. The effectiveness of the learned embeddings is evaluated within multiple data mining tasks, including word analogy, text classification, and node classification. The results suggest that the LINE model outperforms other competitive baselines in terms of both effectiveness and efficiency. It is able to learn the embedding of a network with millions of nodes and billions of edges in a few hours on a single machine.": "这条线是非常通用的，它适用于有向或无向、加权或非加权图。我们用各种真实世界的信息网络，包括语言网络、社交网络和引文网络来评估这条线路的性能。在多个数据挖掘任务(包括单词类推、文本分类和节点分类)中评估所学习的嵌入的有效性。结果表明，该线性模型在有效性和效率方面均优于其他竞争基线。它能够在一台机器上几个小时内学习数百万个节点和数十亿个边的网络的嵌入。",
    "QLabel": "label标签",
    "self.centralwidget": "Sel.Central小部件",
    "\"QLabel{color:rgb(225,22,173,255);font-size:50px;font-weight:normal;font-family:Arial;}\"": "“QLabel{color:rgb(225，22，173，255)；font-size:50px；font-weight:normal；font-family:Arial；}”",
    "self.textEdit.": "文本编辑。",
    "(\"QLabel{color:rgb(225,22,173,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "(“QLabel{color:rgb(225，22，173，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "\n        # self.centralwidget.setStyleSheet(\"QLabel{color:rgb(225,22,173,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "#self.centralwidget.setStyleSheet(“QLabel{color:rgb(225，22，173，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "         # self.centralwidget.setStyleSheet(\"QLabel{color:rgb(225,22,173,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "#self.centralwidget.setStyleSheet(“QLabel{color:rgb(225，22，173，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "255": "二百五十五",
    ".setStyleSheet(\"QLabel{color:rgb(0,0,0,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": ".setStyleSheet(“QLabel{color:rgb(0，0，0，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "normal": "adj. 正常的;正规的，标准的;[数学]正交的;精神健全的;\nn. 正常，常态;标准;[数学]法线;",
    "setStyleSheet": "setStyleSheet",
    "setObjectName": "setObjectName",
    "setObjectNamesetObjectName": "setObjectNamesetObjectName",
    "setObjectNamesadf": "Setobjectnames同盟军",
    "setObjectNamesadfasdf": "setobjectnamesAdFDF",
    "asd": "abbr. atrial septal defect 房间隔缺损;aldosterone secretion defect 醛固酮分泌缺损;air surveillance drone 航空监督标志;avionic system division 航电系统分支;",
    "asdfvcawefv": "asdfvcawefv",
    "asdfvcawefvzxcv": "asdfvcawefvzxcv",
    "textEdit_2": "textedtext（发短信）的过去式与过去分词形式",
    "QTextEdit": "QTextEdit",
    "df": " dead freight 空舱费;",
    "dfvsad": "dfvsad",
    "pyqt5 QTextEdit ": "pyqt 5 QTextEdit",
    "\n        self.textEdit_3.setStyleSheet(\"QLabel{color:rgb(0,0,0,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "self.textEdit_3.setStyleSheet(“QLabel{color:rgb(0，0，0，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "         self.textEdit_3.setStyleSheet(\"QLabel{color:rgb(0,0,0,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "self.textEdit_3.setStyleSheet(“QLabel{color:rgb(0，0，0，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "\nself.textEdit_3.setStyleSheet(\"QLabel{color:rgb(0,0,0,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "self.textEdit_3.setStyleSheet(“QLabel{color:rgb(0，0，0，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    " self.textEdit_3.setStyleSheet(\"QLabel{color:rgb(0,0,0,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "self.textEdit_3.setStyleSheet(“QLabel{color:rgb(0，0，0，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "cursor = self.editor.textCursor()": "游标=auto.Editor or.textCursor()",
    "edit": "vt. 剪辑（影片，录音）;编辑;校订;主编;\nn. 编辑;",
    "self.textEdit_3": "自.textEdit_3",
    "self.textEdit_2.setStyleSheet(\"QLabel{color:rgb(0,0,0,255);font-size:20px;font-weight:normal;font-family:Arial;}\")": "self.textEdit_2.setStyleSheet(“QLabel{color:rgb(0，0，0，255)；font-size:20px；font-weight:normal；font-family:Arial；}”)",
    "setFontPointSize": "setFontPointSize",
    "\n        self.textEdit_3.selectAll()\n        self.textEdit_2.setFontPointSize(20)\n        self.textEdit_2.setTextCursor()": "\"",
    "         self.textEdit_3.selectAll()         self.textEdit_2.setFontPointSize(20)         self.textEdit_2.setTextCursor()": "\"",
    "edit.": "abbr. edition 版（次）;版本;editor 编辑;主编;",
    "\n": "",
    " ": "",
    "cursor": "n. 光标;",
    "fontsize": "字号",
    "changeFontSize": "changeFontSize",
    "self.textEdit_2.": "-.",
    "self.changeFontSize(self.textEdit_2)": "self.changeFontSize(self.textEdit_2)",
    "textEdit_3": "textedtext（发短信）的过去式与过去分词形式",
    "size": "n. 大小，尺寸;规模;胶料，浆糊;巨大，大量;\nvt. 按大小排列;改变…的大小;上胶料，上浆;上涂料;\nadj. 一定尺寸的;",
    "source": "n. 根源，本源;源头，水源;原因;提供消息的人;\nvt. （从…）获得;发起;向…提供消息;寻求（尤指供货）的来源;\nvi. 原料来源;起源;寻求来源;寻求生产商（或提供商）;",
    "sourcesource": "no resource无路可走",
    "textEdit": "textedtext（发短信）的过去式与过去分词形式",
    "Form": "n. 形状，形式;外形;方式;表格;\nvt. 形成;构成;组织;塑造;\nvi. 形成，产生;排队，整队;",
    "load": "n. 负荷;负担;装载;工作量;\nvt. 使担负;装填;把…装入或装上;装满，堆积;\nvi. 加载;装载;装货;",
    "ciba.py": "雪碧",
    "We propose a novel network embedding model called\nthe “LINE,” which suits arbitrary types of information\nnetworks and easily scales to millions of nodes. It has\na carefully designed objective function that preserves\nboth the first-order and second-order proximities.": "我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且易于扩展到数百万个节点。它有一个精心设计的目标函数，它既保留了一阶近似值，也保留了二阶近似值.",
    "We propose a novel network embedding model called the “LINE,” which suits arbitrary types of information networks and easily scales to millions of nodes. It has a carefully designed objective function that preserves both the first-order and second-order proximities.": "我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且易于扩展到数百万个节点。它有一个精心设计的目标函数，它既保留了一阶近似值，也保留了二阶近似值.",
    "We propose a novel network embedding model called the “LINE,” which suits arbitrary types of information networks and easily scales to millions of nodes. It has a carefully designed objective function that preserves both the first-order and second-order proximities.We propose a novel network embedding model called\nthe “LINE,” which suits arbitrary types of information\nnetworks and easily scales to millions of nodes. It has\na carefully designed objective function that preserves\nboth the first-order and second-order proximities.We propose a novel network embedding model called\nthe “LINE,” which suits arbitrary types of information\nnetworks and easily scales to millions of nodes. It has\na carefully designed objective function that preserves\nboth the first-order and second-order proximities.": "我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且易于扩展到数百万个节点。它有一个精心设计的目标函数，保留了一阶和二阶近似，我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且很容易扩展到数百万个节点。它有一个精心设计的目标函数，保留了一阶和二阶近似，我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且很容易扩展到数百万个节点。它有一个精心设计的目标函数，它既保留了一阶近似值，也保留了二阶近似值.",
    "We propose a novel network embedding model called the “LINE,” which suits arbitrary types of information networks and easily scales to millions of nodes. It has a carefully designed objective function that preserves both the first-order and second-order proximities.We propose a novel network embedding model called the “LINE,” which suits arbitrary types of information networks and easily scales to millions of nodes. It has a carefully designed objective function that preserves both the first-order and second-order proximities.We propose a novel network embedding model called the “LINE,” which suits arbitrary types of information networks and easily scales to millions of nodes. It has a carefully designed objective function that preserves both the first-order and second-order proximities.": "我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且易于扩展到数百万个节点。它有一个精心设计的目标函数，保留了一阶和二阶近似，我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且很容易扩展到数百万个节点。它有一个精心设计的目标函数，保留了一阶和二阶近似，我们提出了一种新的网络嵌入模型，称为“线”，它适用于任意类型的信息网络，并且很容易扩展到数百万个节点。它有一个精心设计的目标函数，它既保留了一阶近似值，也保留了二阶近似值.",
    "We propose an edge-sampling algorithm for optimizing\nthe objective. The algorithm tackles the limitation of\nthe classical stochastic gradient decent and improves\nthe effectiveness and efficiency of the inference.": "提出了一种优化目标的边缘采样算法.该算法克服了经典随机梯度法的局限性，提高了推理的有效性和效率。",
    "We propose an edge-sampling algorithm for optimizing the objective. The algorithm tackles the limitation of the classical stochastic gradient decent and improves the effectiveness and efficiency of the inference.": "提出了一种优化目标的边缘采样算法.该算法克服了经典随机梯度法的局限性，提高了推理的有效性和效率。",
    "We conduct extensive experiments on real-world infor-\nmation networks. Experimental results prove the ef-\nfectiveness and efficiency of the proposed LINE model.": "我们在现实世界的信息网络上进行了广泛的实验.实验结果证明了该模型的有效性和有效性。",
    "We conduct extensive experiments on real-world information networks. Experimental results prove the effectiveness and efficiency of the proposed LINE model.": "我们在现实世界的信息网络上进行了广泛的实验.实验结果证明了该模型的有效性和有效性。",
    "The rest of this paper is organized as\nfollows. Section 2 summarizes the related work. Section 3\nformally defines the problem of large-scale information net-\nwork embedding. Section 4 introduces the LINE model in\ndetails. Section 5 presents the experimental results. Finally\nwe conclude in Section 6.": "本文的其余部分如下所示。第二节对相关工作进行了总结。第三节对大规模信息网络嵌入问题进行了形式化界定.第四节详细介绍了线路模型。第五节给出了实验结果。最后，我们在第六节中得出结论。",
    "The rest of this paper is organized as follows. Section 2 summarizes the related work. Section 3 formally defines the problem of large-scale information network embedding. Section 4 introduces the LINE model in details. Section 5 presents the experimental results. Finally we conclude in Section 6.": "本文的其余部分如下所示。第二节对相关工作进行了总结。第三节对大规模信息网络嵌入问题进行了形式化界定.第四节详细介绍了线路模型。第五节给出了实验结果。最后，我们在第六节中得出结论。",
    "Our work is related to classical methods of graph em-\nbedding or dimension reduction in general, such as multi-\ndimensional scaling (MDS) [4], IsoMap [20], LLE [18] and\nLaplacian Eigenmap [2]. These approaches typically first\nconstruct the affinity graph using the feature vectors of the\ndata points, e.g., the K-nearest neighbor graph of data, and\nthen embed the affinity graph [22] into a low dimensional\nspace. However, these algorithms usually rely on solving the\nleading eigenvectors of the affinity matrices, the complexity\nof which is at least quadratic to the number of nodes, making\nthem inefficient to handle large-scale networks.": "我们的工作涉及到经典的图嵌入或降维方法，如多维尺度(MDS)[4]、ISOMAP[20]、LLE[18]和Laplacian特征映射[2]。这些方法通常首先使用数据点的特征向量(例如数据的K-最近邻图)构造亲和图，然后将亲和图[22]嵌入到低维空间中。然而，这些算法通常依赖于求解亲和矩阵的主导特征向量，其复杂度至少与节点数成二次，使得它们在处理大规模网络时效率低下。",
    "Our work is related to classical methods of graph embedding or dimension reduction in general, such as multidimensional scaling (MDS) [4], IsoMap [20], LLE [18] and Laplacian Eigenmap [2]. These approaches typically first construct the affinity graph using the feature vectors of the data points, e.g., the K-nearest neighbor graph of data, and then embed the affinity graph [22] into a low dimensional space. However, these algorithms usually rely on solving the leading eigenvectors of the affinity matrices, the complexity of which is at least quadratic to the number of nodes, making them inefficient to handle large-scale networks.": "我们的工作涉及到经典的图嵌入或降维方法，如多维尺度(MDS)[4]、ISOMAP[20]、LLE[18]和Laplacian特征映射[2]。这些方法通常首先使用数据点的特征向量(例如数据的K-最近邻图)构造亲和图，然后将亲和图[22]嵌入到低维空间中。然而，这些算法通常依赖于求解亲和矩阵的主导特征向量，其复杂度至少与节点数成二次，使得它们在处理大规模网络时效率低下。",
    "automatic": "adj. 自动的;不假思索的，无意识的;必然发生的;\nn. 自动化机器或设备;自动手枪;",
    "https://github.com/lkjie/CiBa-translation.git": "https://github.com/lkjie/CiBa-translation.git",
    "/anaconda3/bin/python": "[1][1][1][1][1]",
    "untitled.ui\nuntitled.py\ntranslated.json\ntqt.py\nciba.py": "untled.ui untled.py translated.json tqt.py ciba.py",
    "untitled.ui untitled.py translated.json tqt.py ciba.py": "untled.ui untled.py translated.json tqt.py ciba.py",
    "untitled.ui\nuntitled.py\ntranslated.json\ntqt.py\nciba.py\nlayt1.py": ".json tqt.py ciba.py layt1.py",
    "untitled.ui untitled.py translated.json tqt.py ciba.py layt1.py": ".json tqt.py ciba.py layt1.py",
    "affinity": "n. 密切关系，姻亲关系;（男女之间的）吸引力，吸引人的异性;类同;类似，近似;",
    "Among the most recent literature is a technique called\ngraph factorization [1]. It finds the low-dimensional embed-\nding of a large graph through matrix factorization, which is\noptimized using stochastic gradient descent. This is possi-\nble because a graph can be represented as an affinity ma-\ntrix. However, the objective of matrix factorization is not\ndesigned for networks, therefore does not necessarily pre-\nserve the global network structure. Intuitively, graph fac-\ntorization expects nodes with higher first-order proximity\nare represented closely. Instead, the LINE model uses an\nobjective that is particularly designed for networks, which\npreserves both the first-order and the second-order prox-\nimities. Practically, the graph factorization method only\napplies to undirected graphs while the proposed model is\napplicable for both undirected and directed graphs.": "最近的文献中有一种叫做图因式分解的技巧[1]。通过矩阵分解找到了大图的低维嵌入，并利用随机梯度下降对其进行了优化。这是可能的，因为图可以表示为关联矩阵。然而，矩阵分解的目标并不是针对网络而设计的，因此不一定要保留全局网络结构。直观地说，图因式分解期望具有较高一阶邻近度的节点被紧密地表示.相反，线模型使用了一个专门为网络设计的目标，它既保留了一阶也保留了二阶近似值。实际上，图因式分解方法只适用于无向图，而该模型适用于无向图和有向图。",
    "Among the most recent literature is a technique called graph factorization [1]. It finds the low-dimensional embedding of a large graph through matrix factorization, which is optimized using stochastic gradient descent. This is possible because a graph can be represented as an affinity matrix. However, the objective of matrix factorization is not designed for networks, therefore does not necessarily preserve the global network structure. Intuitively, graph factorization expects nodes with higher first-order proximity are represented closely. Instead, the LINE model uses an objective that is particularly designed for networks, which preserves both the first-order and the second-order proximities. Practically, the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs.Among the most recent literature is a technique called\ngraph factorization [1]. It finds the low-dimensional embed-\nding of a large graph through matrix factorization, which is\noptimized using stochastic gradient descent. This is possi-\nble because a graph can be represented as an affinity ma-\ntrix. However, the objective of matrix factorization is not\ndesigned for networks, therefore does not necessarily pre-\nserve the global network structure. Intuitively, graph fac-\ntorization expects nodes with higher first-order proximity\nare represented closely. Instead, the LINE model uses an\nobjective that is particularly designed for networks, which\npreserves both the first-order and the second-order prox-\nimities. Practically, the graph factorization method only\napplies to undirected graphs while the proposed model is\napplicable for both undirected and directed graphs.": "最近的文献中有一种叫做图因式分解的技巧[1]。通过矩阵分解找到了大图的低维嵌入，并利用随机梯度下降对其进行了优化。这是可能的，因为图可以表示为关联矩阵。然而，矩阵分解的目标并不是针对网络而设计的，因此不一定要保留全局网络结构。直观地说，图因式分解期望具有较高一阶邻近度的节点被紧密地表示.相反，线模型使用了一个专门为网络设计的目标，它既保留了一阶也保留了二阶近似值。实际上，图因式分解方法只适用于无向图，而该模型适用于无向图和有向图。最近的文献中有一种叫做图因式分解的技术[1]。通过矩阵分解找到了大图的低维嵌入，并利用随机梯度下降对其进行了优化。这是可能的，因为图可以表示为关联矩阵。然而，矩阵分解的目标并不是针对网络而设计的，因此不一定要保留全局网络结构。直观地说，图因式分解期望具有较高一阶邻近度的节点被紧密地表示.相反，线模型使用了一个专门为网络设计的目标，它既保留了一阶也保留了二阶近似值。实际上，图因式分解方法只适用于无向图，而该模型适用于无向图和有向图。",
    "Among the most recent literature is a technique called graph factorization [1]. It finds the low-dimensional embedding of a large graph through matrix factorization, which is optimized using stochastic gradient descent. This is possible because a graph can be represented as an affinity matrix. However, the objective of matrix factorization is not designed for networks, therefore does not necessarily preserve the global network structure. Intuitively, graph factorization expects nodes with higher first-order proximity are represented closely. Instead, the LINE model uses an objective that is particularly designed for networks, which preserves both the first-order and the second-order proximities. Practically, the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs.Among the most recent literature is a technique called graph factorization [1]. It finds the low-dimensional embedding of a large graph through matrix factorization, which is optimized using stochastic gradient descent. This is possible because a graph can be represented as an affinity matrix. However, the objective of matrix factorization is not designed for networks, therefore does not necessarily preserve the global network structure. Intuitively, graph factorization expects nodes with higher first-order proximity are represented closely. Instead, the LINE model uses an objective that is particularly designed for networks, which preserves both the first-order and the second-order proximities. Practically, the graph factorization method only applies to undirected graphs while the proposed model is applicable for both undirected and directed graphs.": "最近的文献中有一种叫做图因式分解的技巧[1]。通过矩阵分解找到了大图的低维嵌入，并利用随机梯度下降对其进行了优化。这是可能的，因为图可以表示为关联矩阵。然而，矩阵分解的目标并不是针对网络而设计的，因此不一定要保留全局网络结构。直观地说，图因式分解期望具有较高一阶邻近度的节点被紧密地表示.相反，线模型使用了一个专门为网络设计的目标，它既保留了一阶也保留了二阶近似值。实际上，图因式分解方法只适用于无向图，而该模型适用于无向图和有向图。最近的文献中有一种叫做图因式分解的技术[1]。通过矩阵分解找到了大图的低维嵌入，并利用随机梯度下降对其进行了优化。这是可能的，因为图可以表示为关联矩阵。然而，矩阵分解的目标并不是针对网络而设计的，因此不一定要保留全局网络结构。直观地说，图因式分解期望具有较高一阶邻近度的节点被紧密地表示.相反，线模型使用了一个专门为网络设计的目标，它既保留了一阶也保留了二阶近似值。实际上，图因式分解方法只适用于无向图，而该模型适用于无向图和有向图。",
    "The most recent work related with ours is DeepWalk [16],\nwhich deploys a truncated random walk for social network\nembedding. Although empirically effective, the DeepWalk\ndoes not provide a clear objective that articulates what net-\nwork properties are preserved. Intuitively, DeepWalk ex-\npects nodes with higher second-order proximity yield similar\nlow-dimensional representations, while the LINE preserves\nboth first-order and second-order proximities. DeepWalk\nuses random walks to expand the neighborhood of a vertex,\nwhich is analogical to a depth-first search. We use a breadth-\nfirst search strategy, which is a more reasonable approach to\nthe second-order proximity. Practically, DeepWalk only ap-\nplies to unweighted networks, while our model is applicable\nfor networks with both weighted and unweighted edges.": "与我们相关的最新工作是DeepWalk[16]，它为社会网络嵌入部署了一个截断的随机游动。尽管从经验上来说是有效的，但DeepWalk并没有提供一个清晰的目标来阐明哪些网络属性被保留。从直觉上看，DeepWalk期望具有较高二阶邻近度的节点产生类似的低维表示，而这条线同时保留一阶和二阶近似值。DeepWalk使用随机游动来扩展顶点的邻域，这类似于深度优先搜索。我们使用了一种广度优先搜索策略，这是一种更合理的方法来接近第二阶。实际上，DeepWalk只适用于非加权网络，而我们的模型适用于具有加权边和无权边的网络。",
    "The most recent work related with ours is DeepWalk [16], which deploys a truncated random walk for social network embedding. Although empirically effective, the DeepWalk does not provide a clear objective that articulates what network properties are preserved. Intuitively, DeepWalk expects nodes with higher second-order proximity yield similar low-dimensional representations, while the LINE preserves both first-order and second-order proximities. DeepWalk uses random walks to expand the neighborhood of a vertex, which is analogical to a depth-first search. We use a breadthfirst search strategy, which is a more reasonable approach to the second-order proximity. Practically, DeepWalk only applies to unweighted networks, while our model is applicable for networks with both weighted and unweighted edges.": "与我们相关的最新工作是DeepWalk[16]，它为社会网络嵌入部署了一个截断的随机游动。尽管从经验上来说是有效的，但DeepWalk并没有提供一个清晰的目标来阐明哪些网络属性被保留。从直觉上看，DeepWalk期望具有较高二阶邻近度的节点产生类似的低维表示，而这条线同时保留一阶和二阶近似值。DeepWalk使用随机游动来扩展顶点的邻域，这类似于深度优先搜索。我们使用了一种广度优先搜索策略，这是一种更合理的方法来接近第二阶。实际上，DeepWalk只适用于非加权网络，而我们的模型适用于具有加权边和无权边的网络。",
    "In Section 5, we empirically compare the proposed model\nwith these methods using various real world networks.": "在第五节中，我们将所提出的模型与使用各种真实世界网络的方法进行了实证比较。",
    "In Section 5, we empirically compare the proposed model with these methods using various real world networks.": "在第五节中，我们将所提出的模型与使用各种真实世界网络的方法进行了实证比较。",
    "'translated.json'": "“Translated.json”",
    "We formally define the problem of large-scale information\nnetwork embedding using first-order and second-order prox-\nimities. We first define an information network as follows:": "我们正式定义了利用一阶和二阶近似值进行大规模信息网络嵌入的问题.我们首先将信息网络定义如下：",
    "We formally define the problem of large-scale information network embedding using first-order and second-order proximities. We first define an information network as follows:": "我们正式定义了利用一阶和二阶近似值进行大规模信息网络嵌入的问题.我们首先将信息网络定义如下：",
    "An informa-\ntion network is defined as G = (V, E), where V is the set\nof vertices, each representing a data object and E is the\nset of edges between the vertices, each representing a re-\nlationship between two data objects. Each edge e ∈ E is\nan ordered pair e = (u, v) and is associated with a weight\nw uv > 0, which indicates the strength of the relation. If G\nis undirected, we have (u, v) ≡ (v, u) and w uv ≡ w vu ; if G\nis directed, we have (u, v) 6≡ (v, u) and w uv 6≡ w vu .": "信息网络定义为G=(V，E)，其中V是顶点集合，每个顶点代表一个数据对象，E表示顶点之间的边集，每个边表示两个数据对象之间的关系。每个边e∈E是一个有序对e=(u，v)，并与重量wUV>0相关联，这表明了这种关系的强度。如果G是无向的，则我们有(u，v)≡(v，u)和w uv≡w vu；如果G是有向的，则我们有(u，v)6≡(v，u)和w uv 6≡w vu。",
    "An information network is defined as G = (V, E), where V is the set of vertices, each representing a data object and E is the set of edges between the vertices, each representing a relationship between two data objects. Each edge e ∈ E is an ordered pair e = (u, v) and is associated with a weight w uv > 0, which indicates the strength of the relation. If G is undirected, we have (u, v) ≡ (v, u) and w uv ≡ w vu ; if G is directed, we have (u, v) 6≡ (v, u) and w uv 6≡ w vu .": "信息网络定义为G=(V，E)，其中V是顶点集合，每个顶点代表一个数据对象，E表示顶点之间的边集，每个边表示两个数据对象之间的关系。每个边e∈E是一个有序对e=(u，v)，并与重量wUV>0相关联，这表明了这种关系的强度。如果G是无向的，则我们有(u，v)≡(v，u)和w uv≡w vu；如果G是有向的，则我们有(u，v)6≡(v，u)和w uv 6≡w vu。",
    "In practice, information networks can be either directed\n(e.g., citation networks) or undirected (e.g., social network\nof users in Facebook). The weights of the edges can be either\nbinary or take any real value. Note that while negative edge\nweights are possible, in this study we only consider non-\nnegative weights. For example, in citation networks and\nsocial networks, w uv takes binary values; in co-occurrence\nnetworks between different objects, w uv can take any non-\nnegative value. The weights of the edges in some networks\nmay diverge as some objects co-occur many times while oth-\ners may just co-occur a few times.": "在实践中，信息网络可以是定向的(例如引文网络)，也可以是无指向的(例如，Facebook中用户的社交网络)。边的权重可以是二进制的，也可以取任何实数。注意，虽然负边权是可能的，但在本研究中，我们只考虑非负权。例如，在引用网络和社交网络中，w uv取二进制值；在不同对象之间的共现网络中，w uv可以取任何非负值。在某些网络中，边的权重可能会发生差异，因为一些对象可能会发生多次，而另一些对象可能只会发生几次。",
    "In practice, information networks can be either directed (e.g., citation networks) or undirected (e.g., social network of users in Facebook). The weights of the edges can be either binary or take any real value. Note that while negative edge weights are possible, in this study we only consider nonnegative weights. For example, in citation networks and social networks, w uv takes binary values; in co-occurrence networks between different objects, w uv can take any nonnegative value. The weights of the edges in some networks may diverge as some objects co-occur many times while others may just co-occur a few times.": "在实践中，信息网络可以是定向的(例如引文网络)，也可以是无指向的(例如，Facebook中用户的社交网络)。边的权重可以是二进制的，也可以取任何实数。注意，虽然负边权是可能的，但在本研究中，我们只考虑非负权。例如，在引用网络和社交网络中，w uv取二进制值；在不同对象之间的共现网络中，w uv可以取任何非负值。在某些网络中，边的权重可能会发生差异，因为一些对象可能会发生多次，而另一些对象可能只会发生几次。",
    "Embedding an information network into a low-dimensional\nspace is useful in a variety of applications. To conduct the\nembedding, the network structures must be preserved. The\nfirst intuition is that the local network structure, i.e., the\nlocal pairwise proximity between the vertices, must be pre-\nserved. We define the local network structures as the first-\norder proximity between the vertices:": "将信息网络嵌入到低维空间中在各种应用中都很有用。为了进行嵌入，必须保留网络结构。第一种直觉是，必须保持局部网络结构，即顶点之间的局部配对邻近。我们将局部网络结构定义为顶点之间的一阶邻近性：",
    "Embedding an information network into a low-dimensional space is useful in a variety of applications. To conduct the embedding, the network structures must be preserved. The first intuition is that the local network structure, i.e., the local pairwise proximity between the vertices, must be preserved. We define the local network structures as the firstorder proximity between the vertices:": "将信息网络嵌入到低维空间中在各种应用中都很有用。为了进行嵌入，必须保留网络结构。第一种直觉是，必须保持局部网络结构，即顶点之间的局部配对邻近。我们将局部网络结构定义为顶点之间的一阶邻近性：",
    "The first-order\nproximity in a network is the local pairwise proximity be-\ntween two vertices. For each pair of vertices linked by an\nedge (u, v), the weight on that edge, w uv , indicates the first-\norder proximity between u and v. If no edge is observed\nbetween u and v, their first-order proximity is 0.": "网络中的一阶邻近是两个顶点之间的局部成对邻近.对于每一对由边(u，v)连接的顶点，该边上的权重wUV表示u和v之间的一阶接近度，如果在u和v之间没有观察到边，则它们的一阶接近度为0。",
    "The first-order proximity in a network is the local pairwise proximity between two vertices. For each pair of vertices linked by an edge (u, v), the weight on that edge, w uv , indicates the firstorder proximity between u and v. If no edge is observed between u and v, their first-order proximity is 0.": "网络中的一阶邻近是两个顶点之间的局部成对邻近.对于每一对由边(u，v)连接的顶点，该边上的权重wUV表示u和v之间的一阶接近度，如果在u和v之间没有观察到边，则它们的一阶接近度为0。",
    "The first-order proximity usually implies the similarity of\ntwo nodes in a real-world network. For example, people who\nare friends with each other in a social network tend to share\nsimilar interests; pages linking to each other in World Wide\nWeb tend to talk about similar topics. Because of this im-\nportance, many existing graph embedding algorithms such": "一阶邻近通常意味着现实世界网络中两个节点的相似性.例如，在社交网络中彼此是朋友的人往往有着相似的兴趣；在万维网上相互链接的页面则倾向于谈论类似的话题。由于这一重要性，许多现有的图嵌入算法如",
    "The first-order proximity usually implies the similarity of two nodes in a real-world network. For example, people who are friends with each other in a social network tend to share similar interests; pages linking to each other in World Wide Web tend to talk about similar topics. Because of this importance, many existing graph embedding algorithms such": "一阶邻近通常意味着现实世界网络中两个节点的相似性.例如，在社交网络中彼此是朋友的人往往有着相似的兴趣；在万维网上相互链接的页面则倾向于谈论类似的话题。由于这一重要性，许多现有的图嵌入算法如",
    "\n# size\nLABEL_STRAT_X = 20\nLABEL_STRAT_Y = 20\nLABEL_WIDTH = 200\nLABEL_HEIGTH = 20\nTEXTSOURCE_WIDTH = 600\nTEXTSOURCE_HEIGTH = 200\nBUTTON_WIDTH = 150\nBUTTON_HEIGTH = 80\nTEXTTRANS_WIDTH = TEXTSOURCE_WIDTH + BUTTON_WIDTH * 1.2\nTEXTTRANS_HEIGTH = TEXTSOURCE_HEIGTH + BUTTON_HEIGTH * 1.2": "#Ssize Label_strat_X=20 Label_strat_Y=20 Label_HIGTH=20 TEXTSOURCE_HEIGTH=600 TEXTSOURCE_HEIGTH=200 BUSTROCE_HIGTH=150 BLECT_HEIGTH=80 TEXTRANS_WITH=TEXTSOURCE_WARD_WARD*1.2 TEXTTRANS_HEIGTH=TEXTSOURCE_HEIGTH按钮_HEIGTH*1.2 TEXTSOURCE_HEIGTH按钮",
    "\n\n": "",
    "  ": "",
    "Ui_Form": "UI表格",
    "CiBaTran": "CiBaTran",
    "\n        QtCore.QMetaObject.connectSlotsByName(Form)\n\n        self.pushButton.clicked.connect(Form.trans)\n        self.pushButton_2.clicked.connect(self.textEdit_1.clear)\n        QtCore.QMetaObject.connectSlotsByName(Form)\n\n        self.setWindowFlags(QtCore.Qt.WindowMinimizeButtonHint |  # 使能最小化按钮\n                            QtCore.Qt.WindowCloseButtonHint |  # 使能关闭按钮\n                            QtCore.Qt.WindowStaysOnTopHint)  # 窗体总在最前端": "QtCore.QMetaObject.connectSlotsByName (Form) self.pushButton.clicked.connect (Form.trans) self.pushButton_2.clicked.connect (self.textEdit_1.clear) QtCore.QMetaObject.connectSlotsByName (Form) self.setWindowFlags (QtCore.) Qt.WindowMinimizeButtonHint # enable minimize buttons QtCore.Qt.WindowCloseButtonHint # enable to close buttons QtCore.Qt.WindowStaysOnTopHint) # forms are always in the front end",
    "         QtCore.QMetaObject.connectSlotsByName(Form)          self.pushButton.clicked.connect(Form.trans)         self.pushButton_2.clicked.connect(self.textEdit_1.clear)         QtCore.QMetaObject.connectSlotsByName(Form)          self.setWindowFlags(QtCore.Qt.WindowMinimizeButtonHint |  # 使能最小化按钮                             QtCore.Qt.WindowCloseButtonHint |  # 使能关闭按钮                             QtCore.Qt.WindowStaysOnTopHint)  # 窗体总在最前端": "QtCore.QMetaObject.connectSlotsByName (Form) self.pushButton.clicked.connect (Form.trans) self.pushButton_2.clicked.connect (self.textEdit_1.clear) QtCore.QMetaObject.connectSlotsByName (Form) self.setWindowFlags (QtCore.) Qt.WindowMinimizeButtonHint # enable minimize buttons QtCore.Qt.WindowCloseButtonHint # enable to close buttons QtCore.Qt.WindowStaysOnTopHint) # forms are always in the front end",
    "self": "n. 自己;本人;本性;私利;\nadj. 同一的;纯净的;单一的;\nvt. 使自花授精;使近亲繁殖;\nvi. 自花授精;",
    "Ui_CiBaTran": "UI_CiBaTran",
    "QMetaObject": "QMetaObject",
    "QMetaObjectQMetaObject": "QMetaObjectQMetaObject",
    "_translate(\"CiBaTran\", \"翻译结果\")": "_ translate (\"CiBaTran\", \"Translation result\")",
    "\n        # 等待5秒后，给触发信号，并传递test": "# after waiting for 5 seconds, give the trigger signal and pass the test",
    "         # 等待5秒后，给触发信号，并传递test": "# after waiting for 5 seconds, give the trigger signal and pass the test",
    "changeDetect": "changeDetect",
    "textData": "test data测试数据",
    "clipText": "clipText",
    "trigger": "n. （枪）扳机;起动装置，扳柄;引发其他事件的一件事;[电子学]触发器，触发电器;\nvt. 引发，触发;扣…的扳机;发射或使爆炸（武器或爆炸性弹药）;",
    "timer": "n. 定时器;计时器;点火调节装置;跑表，时计;",
    "or clipText in translated.keys() and textData in translated.keys()": "或translated.key()中的clipText和translated.key()中的textData",
    "liwenjie": "liwenjie",
    " // 2": "// 2",
    "\n        window = QtWidgets.QWidget()": "窗口=QtWidgets.QWidget()",
    "         window = QtWidgets.QWidget()": "窗口=QtWidgets.QWidget()",
    "connect": "vt. 连接，联结;使…有联系;为…接通电话;插入插座;\nvi. 连接;建立关系;[体]成功击中;",
    "translated": "v. 翻译( translate的过去式和过去分词 );解释;转化;被翻译;",
    "proximity between a pair of vertices (u, v) in a net-\nwork is the similarity": "网络中一对顶点(u，v)之间的邻近性是相似的。",
    "proximity between a pair of vertices (u, v) in a network is the similarity": "网络中一对顶点(u，v)之间的邻近性是相似的。",
    "The second-\norder proximity between a pair of vertices (u, v) in a net-\nwork is the similarity between their neighborhood network\nstructures. Mathematically, let p u = (w u,1 , . . . , w u,|V | ) de-\nnote the first-order proximity of u with all the other vertices,\nthen the second-order proximity between u and v is deter-\nmined by the similarity between p u and p v . If no vertex is\nlinked from/to both u and v, the second-order proximity\nbetween u and v is 0.": "网络中一对顶点(u，v)之间的二阶邻近性是它们的邻域网络结构之间的相似性。从数学上讲，设pu=(wu，1，。。。表示u与所有其它顶点的一阶邻近度，然后由pu与pv之间的相似性来确定u与v之间的二阶接近。如果没有从u和v连接的顶点，则u和v之间的二阶邻近度为0。",
    "The secondorder proximity between a pair of vertices (u, v) in a network is the similarity between their neighborhood network structures. Mathematically, let p u = (w u,1 , . . . , w u,|V | ) denote the first-order proximity of u with all the other vertices, then the second-order proximity between u and v is determined by the similarity between p u and p v . If no vertex is linked from/to both u and v, the second-order proximity between u and v is 0.": "网络中一对顶点(u，v)之间的二阶邻近性是它们的邻域网络结构之间的相似性。从数学上讲，设pu=(wu，1，。。。表示u与所有其它顶点的一阶邻近度，然后由pu与pv之间的相似性来确定u与v之间的二阶接近。如果没有从u和v连接的顶点，则u和v之间的二阶邻近度为0。",
    "Mathematically": "adv. 算术地;",
    "between": "prep. 在…之间;私下，暗中;在…中任择其一;来往于…之间;\nadv. 当中，中间;",
    "structures": "n. 结构( structure的名词复数 );[生物学]构造;机构;构造物;\nv. 组织( structure的第三人称单数 );安排;制定;",
    "with": "prep. 和，跟;随着;关于;和…一致;",
    "proximity": "n. 接近，邻近;接近度，距离;亲近;",
    "textCursor": "textCurso",
    "tCursor": "cursor光标",
    "self.lastTran": "自我，最后一次",
    "word_trigger": "字触发器",
    "word_trigger1": "字触发器1",
    "word_trigger post": "字触发柱",
    "readme": "n. 自述文件;",
    "(Large-scale Information Network Em-\nbedding) Given a large network G = (V, E), the problem\nof Large-scale Information Network Embedding aims\nto represent each vertex v ∈ V into a low-dimensional space\nR d , i.e., learning a function f G : V → R d , where d \u001c |V |.\nIn the space R d , both the first-order proximity and the\nsecond-order proximity between the vertices are preserved.": "(大规模信息网络嵌入)给定一个大型网络G=(V，E)，大规模信息网络嵌入问题的目的是将每个顶点v∈V表示为一个低维空间Rd，即学习函数fG：v→Rd，其中d_(？)在空间R_d中，保留了顶点之间的一阶邻近和二阶邻近。",
    ", where d \u001c |V |. In the space R d , both the first-order proximity and the second-order proximity between the vertices are preserved.": "，其中D_(V)。在空间R_d中，保留了顶点之间的一阶邻近和二阶邻近。",
    "(Large-scale Information Network Embedding) Given a large network G = (V, E), the problem of Large-scale Information Network Embedding aims to represent each vertex v ∈ V into a low-dimensional space R d , i.e., learning a function f G : V → R d": "(大规模信息网络嵌入)给定一个大型网络G=(V，E)，大规模信息网络嵌入问题的目的是将每个顶点v∈V表示成一个低维空间Rd，即学习函数f G：v→Rd。"
}